# Kubernetes DNS Troubleshooting & GitOps Demo

Hi **gaurav_bohra** sir,
I tried to replicate everything exactly as per your task, especially the requirement:
> **Modify CoreDNS or create a network policy that breaks service discovery**  
> **Diagnose with tools like nslookup, dig, tcpdump, and restore functionality**

Here are the detailed steps I followed. This is not based on my own interpretation â€” every folder and file was created to fully match the intent of your task:

---

## âœ… Step-by-Step Breakdown

### ğŸ§± 1. `charts/my-app/`
Contains a **Helm chart** that defines:
- **Backend Deployment & Service** using [`http-echo`](https://github.com/hashicorp/http-echo)
- **Frontend Deployment & Service** using NGINX
- **Ingress** that routes:
  - `/api` â†’ `backend-service`
  - `/ui`  â†’ `frontend-service`
- TLS enabled via a secret named `tls-cert`

ğŸ“Œ **Purpose**: Declarative, reusable way to deploy the application using Helm.

---

### ğŸ“¦ 2. `argocd/app.yaml`
ArgoCD Application manifest configured with:
- ğŸ”„ **Auto-sync**
- ğŸ§¹ **Prune**
- â¤ï¸ **Self-heal**
- ğŸ¯ Source points to the Helm chart in this repo

ğŸ“Œ **Purpose**: Manage deployments via GitOps using ArgoCD.

---

### ğŸ’¥ 3. `tools/coredns-patch.yaml`
Patches CoreDNS `Corefile` to **break Kubernetes service discovery**:
- Removes internal Kubernetes DNS plugin block

ğŸ“Œ **Purpose**: Simulate a DNS failure scenario from inside the cluster.

---

### ğŸ” 4. `tools/network-policy-block.yaml`
Creates a **NetworkPolicy** to block:
- **UDP traffic on port 53** (DNS)
- Target: All pods attempting to reach CoreDNS in `kube-system` namespace

ğŸ“Œ **Purpose**: Another way to simulate DNS resolution failure.

---

### ğŸ” 5. `tools/diagnosis.md`
Step-by-step instructions to debug DNS issues using:
- ğŸ” `nslookup`
- ğŸ§  `dig`
- ğŸ“¡ `tcpdump`

Also includes recovery steps:
- Restart CoreDNS
- Delete the blocking NetworkPolicy

ğŸ“Œ **Purpose**: Help diagnose and restore broken DNS functionality.

---

### ğŸš« 6. `misconfigs/values-bad.yaml`
Intentionally invalid Helm values:
```yaml
replicaCount: "three"  # âŒ should be an integer
image:
  tag: "nonexistent-v1.0"  # âŒ invalid image tag
resources:
  limits:
    cpu: "10cores"  # âŒ invalid CPU format
service:
  port: "eighty"  # âŒ should be an integer
```

ğŸ“Œ **Purpose**: Simulate deployment failures for ArgoCD rollback testing.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ charts/
â”‚   â””â”€â”€ my-app/
â”‚       â”œâ”€â”€ Chart.yaml
â”‚       â”œâ”€â”€ values.yaml
â”‚       â””â”€â”€ templates/
â”‚           â”œâ”€â”€ backend-deployment.yaml
â”‚           â”œâ”€â”€ backend-service.yaml
â”‚           â”œâ”€â”€ frontend-deployment.yaml
â”‚           â”œâ”€â”€ frontend-service.yaml
â”‚           â”œâ”€â”€ ingress.yaml
â”‚           â””â”€â”€ tls-secret.yaml
â”œâ”€â”€ argocd/
â”‚   â””â”€â”€ app.yaml
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ coredns-patch.yaml
â”‚   â”œâ”€â”€ network-policy-block.yaml
â”‚   â”œâ”€â”€ debug-pod.yaml
â”‚   â””â”€â”€ diagnosis.md
â””â”€â”€ misconfigs/
    â””â”€â”€ values-bad.yaml
```

## ğŸš€ Implementation Steps

### 1. Deploy the Application
```bash
kubectl apply -f argocd/app.yaml
argocd app wait my-app --sync
kubectl get pods,svc,ingress -n default
```

### 2. Break DNS Resolution
```bash
# Method 1: CoreDNS Configuration
kubectl apply -f tools/coredns-patch.yaml
kubectl rollout restart deployment/coredns -n kube-system

# Method 2: NetworkPolicy Block
kubectl apply -f tools/network-policy-block.yaml
kubectl apply -f tools/debug-pod.yaml
```

### 3. Diagnose with Tools
```bash
kubectl exec -it debug-pod -- bash

# nslookup testing
nslookup backend-service
nslookup backend-service.default.svc.cluster.local

# dig analysis
dig @10.96.0.10 backend-service.default.svc.cluster.local
dig backend-service.default.svc.cluster.local +trace

# tcpdump capture
tcpdump -i any -n port 53 -A
tcpdump -i any -n host backend-service
```

### 4. Restore Functionality
```bash
# Restore CoreDNS
kubectl rollout undo deployment/coredns -n kube-system

# Remove NetworkPolicy
kubectl delete -f tools/network-policy-block.yaml

# Verify Recovery
kubectl exec -it debug-pod -- nslookup backend-service
```

### 5. ArgoCD Rollback Demo
```bash
# Apply broken configuration
argocd app set my-app --values-literal-file misconfigs/values-bad.yaml

# Check application health
argocd app get my-app

# Perform rollback
argocd app history my-app
argocd app rollback my-app <REVISION-ID>
argocd app sync my-app
```

## ğŸ”§ Troubleshooting Commands

### DNS Resolution Testing
```bash
nslookup backend-service
dig backend-service.default.svc.cluster.local
host backend-service.default.svc.cluster.local
```

### Network Connectivity
```bash
telnet backend-service 80
nc -zv backend-service 80
curl backend-service
```

### Packet Capture
```bash
tcpdump -i any -n port 53
tcpdump -i any -n host backend-service
tcpdump -i any -n -vv 'port 53 or port 80'
```

### CoreDNS Investigation
```bash
kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl logs -n kube-system -l k8s-app=kube-dns
kubectl get configmap coredns -n kube-system -o yaml
```

This implementation exactly matches your requirements for breaking service discovery, diagnosing with standard tools, and demonstrating GitOps rollback capabilities.
